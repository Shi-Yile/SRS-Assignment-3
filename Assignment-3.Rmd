---
title: "Statistical Research Skills - Assignment 3"
author: "Yile Shi"
date: "2022/4/18"
output: 
  pdf_document:
    fig_caption: true
bibliography: reference.bib
nocite: |
  @scott2009ash, @gu2011gss
csl: journal-of-statistical-computation-and-simulation.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# install.packages("gss")
# install.packages("kdensity")
# install.packages("ash")
library(tidyverse)
library(gss)
library(kdensity)
library(ash)
library(ggplot2)
```

## 1. Introduction

In this report, we compare the performance of kernel density estimation with other density estimators. To this end, our report consists of two main sections. In the first section, we run a one-shot experiment using kernel density estimation and its competitors, including average shifted histogram and penalized likelihood estimation, based on specified data sequences. In the second part, we conduct a Monte Carlo simulation study for different sample sizes and observe how estimation accuracy changes when we increase the sample size by reporting the corresponding integrated squared errors. 

## 2. Data Generating Processes and Preliminary Experiments

In this section, we introduce the methodologies of kernel density estimation and its competitors first. Next, we run one-shot experiments and construct density plots to compare the estimation performance, after data generating processes.

### 2.1 Methodology

**Kernel Density Estimation**
  
Kernel density estimation (KDE) is a widely-used density estimator, which centres a smooth kernel function at each data point then summing to get a density estimate [@deng2011density]. 

Let $X_1, \cdots, X_n \sim^{iid} f$. The kernel estimator of $f$ is defined as 
$$\hat{f}_{kde}(x) = \frac{1}{n}\sum\limits_{i=1}^{n}K_h(x - X_i)=\frac{1}{n}\sum\limits_{i=1}^{n}K\bigg(\frac{x-X_i}{h}\bigg) $$
where $K$ is the density kernel and $h$ is the bandwidth [@scott2015multivariate].

The kernel, $K$, is a symmetric, and usually positive function that integrates to one. Common kernel functions are uniform, triangle, Epanechnikov, quartic (biweight), tricube (triweight), Gaussian (normal), and cosine. 

The bandwidth, $h$, is a smoothing parameter. Generally, large bandwidths produce very smooth estimates, while small values produce wiggly estimates. The bandwidth influences estimation accuracy much more than the kernel, so choosing a good bandwidth is critical to get a good estimate.
  
**Average Shifted Histogram**

The histogram is the oldest and least sophisticated method for density estimation. One of its simple enhancement is the average shifted histogram (ASH) [@scott1985averaged], which is smoother and avoids the sensitivity to the choice of origin. Specifically, the premise of this approach is to take $m$ histograms, $\hat{f}_1, \hat{f}_2, \cdots, \hat{f}_m$, of bin width $h$ with origins of $t_o = 0, \frac{h}{m}, \frac{2h}{m}, \cdots, \frac{(m-1)h}{m}$. Based on these, we simply define the naive ASH as:
$$\hat{f}_{ash}(x) =  \frac{1}{m} \sum\limits_{i=1}^{m}\hat{f}_i(x)$$
There are $k = 1, \cdots, m \cdot n$ bins across all histograms, each spanning $\big[k\frac{h}{m}, (k+1)\frac{h}{m}\big]$ with centre $(k+0.5)\frac{h}{m}$. The ASH can be somewhat more general by using all bins to estimate the density at each point, weighting bins closer to the data more highly. The weighted ASH is defined as:
$$\hat{f}_{ash}(x) =  \frac{1}{m} \sum\limits_{k=1}^{m \cdot n}\omega(l_k - x)\hat{c}_k(x)$$
where $\omega$ is a weighting function, $l_k$ is the centre of bin $k$, and $\hat{c}_k(x)$ is the number of points in that bin.
 
**Penalized Likelihood Estimation**

Penalized likelihood estimation (PLE) is another advanced approach to estimate the density, which considered the density as a mixture of $m$ "basis" densities [@schellhase2019density], compromising between estimation accuracy and model complexity. As mentioned, PLE generally approximates the density of $x$, $f(x)$, as a mixture of $m$ densities:
$$\hat{f}_{pen}(x) = \sum\limits_{i=1}^{m}c_i\phi_i(x)$$
where $\phi_i(x)$ is a "basis" density and $c_i$ is the corresponding weight picked to ensure that $\hat{f}_{pen}$ can integrate to $1$. The basis densities are weighted equally and differ only by a location parameter, $\mu_i$. Thus, we can obtain a simplified definition which has a similar format with the kernel approach:
$$\hat{f}_{pen}(x) = \frac{1}{m}\sum\limits_{i=1}^{m}K\bigg(\frac{x-\mu_i}{h}\bigg) $$
Determining the number and location of $\mu_i$ appropriately or not decides the performance of the penalized likelihood estimation. A general way is placing a large number of $\mu_i$ at equally spaced locations along the domain of the data and then minimizing a penalized likelihood to remove $\mu_i$ with little contribution to the overall quality.

### 2.2 Data Generating Processes

To test the performance of aforementioned density estimators on different distributions, we consider data simulation from both symmetric and asymmetric distributions. Specifically, we generate data sequences from the following distributions, without the loss of generality:
\begin{itemize}
  \item Scenario $1$: $Normal(5, 3^2)$, $f(x) = \frac{1}{3\sqrt{2\pi}}e^{-\frac{1}{2}\big(\frac{x-5}{3}\big)^2}$
  \item Scenario $2$: $Beta(3, 8)$ $f(x) = \frac{x^2(1-x)^7}{B(3, 8)}$, where $B(3,8) = \frac{\Gamma(3)\Gamma(8)}{\Gamma(3+8)}$ and $\Gamma$ is the Gamma function
\end{itemize}

```{r echo = FALSE}
# set random seed
set.seed(1)

# data generating for one-shot experiments
data_1 <- rnorm(n = 1000, mean = 5, sd = 3)              # Normal(5, 3^2)
data_2 <- rbeta(n = 1000, shape1 = 3, shape2 = 8)       # Beta(3, 8)
```

### 2.3 One-shot Experiments

Based on previous data simulation, we apply different methods to estimate the densities of data sequences. To keep the inquiry simple, we use the default options for each method. We illustrate the estimated densities in a figure intuitively, as well as the true density and the traditional histogram. We compare the estimation performances and further the strengths and weaknesses of each estimator.

**Scenario 1: Normal distribution**

```{r include = FALSE}
# data_1 ~ Normal(5, 9)
# Average Shifted Histogram
ash_1 <- ash1(bin1(data_1, ab = c(range(data_1)[1], 
                                  range(data_1)[2]), nbin = 50))

# Penalized Likelihood Estimation
ple_1 <- ssden(~ data_1, domain = data.frame(data_1 = c(range(data_1)[1],
                                                        range(data_1)[2])))
```

```{r echo = FALSE, fig.cap = "Estimated densities for Normal(5, 9) with different estimators", fig.height = 6, fig.width = 12}
# visualize the accurate density and estimations
x_1 <- seq(range(data_1)[1], range(data_1)[2], length = 1000)
# histogram
hist(data_1, freq = FALSE, xlab = "x", ylim = c(0, 0.18), main = "")
# accurate density
lines(x_1, dnorm(x_1, mean = 5, sd = 3), lwd = 2, col = "orange")
# Kernel Density Estimates
lines(density(data_1, from = range(data_1)[1], to = range(data_1)[2]), 
      lwd = 2, col = "red")
rug(jitter(data_1), side = 1, col = "red")
# Average Shifted Estimates
lines(ash_1, lwd = 2, col = "green")
# Penalized Likelihood Estimates
lines(x_1, dssden(ple_1, x_1), lwd = 2, col = "blue")
# legend
legend("topright", legend = c("Accurate density", "Kernel density estimation",
                              "Average shifted histogram", 
                              "Penalized density estimation"),
       lwd = c(2, 2, 2, 2), col = c("orange", "red", "green", "blue"), 
       cex = 0.8, bty = "n")
```

According to Figure $1$, we consider the penalized likelihood estimation as the best fitted method in this experiment as its density curve is the smoothest and overlaps the true density most. The kernel density estimator has a quite similar performance with the average shifted histogram. Both of their density curves fall sharply after their peaks.

**Scenario 2: Beta distribution**

```{r include = FALSE}
# data_1 ~ Normal(5, 9)
# Average Shifted Histogram
ash_2 <- ash1(bin1(data_2, ab = c(range(data_2)[1], 
                                  range(data_2)[2]), nbin = 50))

# Penalized Likelihood Estimation
ple_2 <- ssden(~ data_2, domain = data.frame(data_2 = c(range(data_2)[1],
                                                        range(data_2)[2])))
```

```{r echo = FALSE, fig.cap = "Estimated densities for Beta(3, 8) with different estimators", fig.height = 6, fig.width = 12}
# visualize the accurate density and estimations
x_2 <- seq(range(data_2)[1], range(data_2)[2], length = 1000)
# histogram
hist(data_2, freq = FALSE, xlab = "x", ylim = c(0, 4), main = "")
# accurate density
lines(x_2, dbeta(x_2, shape1 = 3, shape2 = 8), lwd = 2, col = "orange")
# Kernel Density Estimates
lines(density(data_2, from = range(data_2)[1], to = range(data_2)[2]), 
      lwd = 2, col = "red")
rug(jitter(data_2), side = 1, col = "red")
# Average Shifted Estimates
lines(ash_2, lwd = 2, col = "green")
# Penalized Likelihood Estimates
lines(x_2, dssden(ple_2, x_2), lwd = 2, col = "blue")
# legend
legend("topright", legend = c("Accurate density", "Kernel density estimation",
                              "Average shifted histogram", 
                              "Penalized density estimation"),
       lwd = c(2, 2, 2, 2), col = c("orange", "red", "green", "blue"), 
       cex = 0.8, bty = "n")
```

From Figure $2$, in this experiment, we observe that all $3$ methods don't fit as well as in the normal scenario. Density curves reach higher peaks than the true density and then drop quickly. Still, PLE performs the best due to its smoothest curve, while the other $2$ curves vary at around $x = 0.4$. 

**Strength and Weakness**

Based on one-shot experiments, we discuss the advantages and disadvantages of each method.

\begin{itemize}
  \item \texttt{Kernel density estimation}: Comparing with histogram approaches, KDE doesn't rely on the the choice of origin and the number of bins, and produces smooth estimated densities. However, KDE depends on the choice of bandwidth much and it's not easy to determine the best bandwidth. Improper bandwidth leads to poor estimation performance. As shown in scenario $2$, the default bandwidth is no longer suitable for asymmetric distributions.
  \item \texttt{Average shifted histogram}: As an improvement of traditional histogram, ASH also avoids the sensitivity of the choice of origin and remains computationally efficient. On the other hand, the performance of ASH is significantly affected by the choice of the number of bins, which is the common weakness of histogram approaches.
  \item \texttt{Penalized likelihood estimation}: The main idea of PLE which estimates the density as a mixture of multiple "basis" densities improves its quality of fit. Moreover, PLE doesn't require "basis" densities to be centred on the data points, compared with KDE, resulting in higher accuracy. As mentioned, one of the weakness of PLE is its high dependence on the location parameter $\mu_i$. 
\end{itemize}

## 3. Monte Carlo Simulation Study

To obtain a robust conclusion on the performance of the kernel density estimator and other competitors, we conduct a Monte Carlo simulation study in this section. 

Specifically, we repeat simulations from $Normal(5, 3^2)$ distribution $100$ times, and in each simulation we generate $250$, $500$ and $1000$ observations. To compare the estimation performance, for each fixed sample size, we compute the integrated square error [@marron1992exact] 
$$ISE = \int\big[\hat{f}_{kde}(x) - f(x) \big]^2dx$$
for each underlying simulated set and report them with boxplots. Moreover, we calculate the overall mean ISE for each sample size with corresponding $95%$ confidence intervals and illustrate the values using a figure.

```{r, include = FALSE}
# data_1 ~ Normal(5, 9)

# function to compute ISE for both density estimation methods
ise_norm <- function(n){
  # This function is defined to get the Integrated Squared Error for both
  # Kernel Density Estimation and Penalized Likelihood Estimation
  # based on data from Normal(5, 9) distribution

  # simulated data from Normal(5, 9)
  x <- rnorm(n, mean = 5, sd = 3)

  # Kernel Density Estimation
  # compute the kernel density estimates
  kde <- kdensity(x, bw = "nrd0", kernel = "gaussian",
                  support = c(min(x), max(x)))
  # compute the corresponding squared errors
  se_kde <- function(x) (kde(x) - dnorm(x, 5, 3))^2
  # compute ISE
  ise_kde <- integrate(se_kde, lower = min(x), upper = max(x))$value

  # Average Shifted Histogram
  # compute the ASH estimates
  ash <- ash1(bin1(x, ab = c(min(x), max(x)), nbin = 50))
  # compute the corresponding squared errors
  se_ash <- splinefun(ash$x, (ash$y - dnorm(ash$x, 5, 3))^2)
  # compute ISE
  ise_ash <- integrate(se_ash, lower = min(x), upper = max(x))$value

  # Penalized Likelihood Estimation
  # compute the penalized likelihood estimates
  ple <- ssden(~x, domain = data.frame(x = c(min(x), max(x))))
  # compute the corresponding squared errors
  se_ple <- function(x) (dssden(ple, x) - dnorm(x, 5, 3))^2
  # compute ISE
  ise_ple <- integrate(se_ple, lower = min(x), upper = max(x))$value

  return(c(ise_kde, ise_ash, ise_ple))

}

set.seed(1)
# sample size = 250
ise_250 <- matrix(0, nrow = 100, ncol = 3)
for (i in 1:100){
  ise_250[i, 1] <- ise_norm(250)[1]
  ise_250[i, 2] <- ise_norm(250)[2]
  ise_250[i, 3] <- ise_norm(250)[3]
}

# sample size = 500
ise_500 <- matrix(0, nrow = 100, ncol = 3)
for (i in 1:100){
  ise_500[i, 1] <- ise_norm(500)[1]
  ise_500[i, 2] <- ise_norm(500)[2]
  ise_500[i, 3] <- ise_norm(500)[3]
}

# sample size = 1000
ise_1000 <- matrix(0, nrow = 100, ncol = 3)
for (i in 1:100){
  ise_1000[i, 1] <- ise_norm(1000)[1]
  ise_1000[i, 2] <- ise_norm(1000)[2]
  ise_1000[i, 3] <- ise_norm(1000)[3]
}
```

```{r, echo = FALSE, fig.cap = "Boxplots for ISE of 3 density estimators", fig.height = 6, fig.width = 12}
# box-plots for ISE based on different methods
par(mfrow = c(1, 3))
boxplot(ise_250 ~ col(ise_250), names = c("KDE", "ASH", "PLE"),
        col = c("gray", "green", "lightblue"), main = "sample size = 250",
        xlab = "estimator", ylab = "ise")
boxplot(ise_500 ~ col(ise_500), names = c("KDE", "ASH", "PLE"),
        col = c("gray", "green", "lightblue"), main = "sample size = 500",
        xlab = "estimator", ylab = "ise")
boxplot(ise_1000 ~ col(ise_1000), names = c("KDE", "ASH", "PLE"),
        col = c("gray", "green", "lightblue"), main = "sample size = 1000",
        xlab = "estimator", ylab = "ise")
```

```{r, echo = FALSE, fig.cap = "Critical values for mean ISEs of different methods", fig.height = 6, fig.width = 12}
# line graphs for mean ISE with CIs
# mean ISE for each method
ise_250_mean <- apply(ise_250, 2, mean)
ise_500_mean <- apply(ise_500, 2, mean)
ise_1000_mean <- apply(ise_1000, 2, mean)

# standard deviations for ISEs
ise_250_sd <- apply(ise_250, 2, sd)
ise_500_sd <- apply(ise_500, 2, sd)
ise_1000_sd <- apply(ise_1000, 2, sd)

# data frame for critical values of ISEs
rep_ise <- data.frame(size = c(250, 500, 1000),
                      mean_kse = c(ise_250_mean[1], ise_500_mean[1],
                                   ise_1000_mean[1]),
                      CI_kse_lower = c(ise_250_mean[1] - 1.96*ise_250_sd[1],
                                       ise_500_mean[1] - 1.96*ise_500_sd[1],
                                       ise_1000_mean[1] - 1.96*ise_1000_sd[1]),
                      CI_kse_upper = c(ise_250_mean[1] + 1.96*ise_250_sd[1],
                                       ise_500_mean[1] + 1.96*ise_500_sd[1],
                                       ise_1000_mean[1] + 1.96*ise_1000_sd[1]),
                      mean_ash = c(ise_250_mean[2], ise_500_mean[2],
                                   ise_1000_mean[2]),
                      CI_ash_lower = c(ise_250_mean[2] - 1.96*ise_250_sd[2],
                                       ise_500_mean[2] - 1.96*ise_500_sd[2],
                                       ise_1000_mean[2] - 1.96*ise_1000_sd[2]),
                      CI_ash_upper = c(ise_250_mean[2] + 1.96*ise_250_sd[2],
                                       ise_500_mean[2] + 1.96*ise_500_sd[2],
                                       ise_1000_mean[2] + 1.96*ise_1000_sd[2]),
                      mean_ple = c(ise_250_mean[3], ise_500_mean[3],
                                   ise_1000_mean[3]),
                      CI_ple_lower = c(ise_250_mean[3] - 1.96*ise_250_sd[3],
                                       ise_500_mean[3] - 1.96*ise_500_sd[3],
                                       ise_1000_mean[3] - 1.96*ise_1000_sd[3]),
                      CI_ple_upper = c(ise_250_mean[3] + 1.96*ise_250_sd[3],
                                       ise_500_mean[3] + 1.96*ise_500_sd[3],
                                       ise_1000_mean[3] + 1.96*ise_1000_sd[3]))

# make the plot
plot <- ggplot(rep_ise, aes(x = size)) +
  geom_point(aes(y = mean_kse, color = "mean ISE for KSE"), size = 1.25) +
  geom_line(aes(y = mean_kse)) +
  geom_ribbon(aes(ymin = CI_kse_lower, ymax = CI_kse_upper,
                  fill = "95% CI for KSE"), alpha = 0.3) +
  geom_point(aes(y = mean_ash, color = "mean ISE for ASH"), size = 1.25) +
  geom_line(aes(y = mean_ash)) +
  geom_ribbon(aes(ymin = CI_ash_lower, ymax = CI_ash_upper,
                  fill = "95% CI for ASH"), alpha = 0.3) +
  geom_point(aes(y = mean_ple, color = "mean ISE for PLE"), size = 1.25) +
  geom_line(aes(y = mean_ple)) +
  geom_ribbon(aes(ymin = CI_ple_lower, ymax = CI_ple_upper,
                  fill = "95% CI for PLE"), alpha = 0.3) +
  labs(x = "sample size", y = "ISE", color = "", fill = "")

plot
```

Generally, lower ISE suggests better performance of the estimator. From Figure $3$, for each sample size, we observe that PLE over-performs other methods with the significantly lower ISEs. KDE shows better performance than ASH when sample size $n=250$, while it is beaten by ASH when we increase the sample size to $500$ and $1000$. Figure $4$ shows consistent results and we observe a decreasing trend of mean ISE for each methods as the sample size increases. Moreover, corresponding confidence intervals also narrow as $n$ increases, indicating improved estimation accuracy.

## 4. Conclusion

In summary, we compare the performance of $3$ density estimators, including kernel density estimation, average shifted histogram and penalized likelihood estimation. For both one-shot experiments and Monte Carlo study, penalized likelihood estimation, a more advanced method, shows the best performance with best fitted curves and lowest ISEs.  

\newpage

## References {-}
