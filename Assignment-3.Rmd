---
title: "Statistical Research Skills - Assignment 3"
author: "Yile Shi"
date: "2022/4/18"
output: 
  pdf_document:
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# install.packages("gss")
# install.packages("kdensity")
# install.packages("ash")
library(tidyverse)
library(gss)
library(kdensity)
library(ash)
library(ggplot2)
```

## 1. Introduction

In this report, we compare the performance of the kernel density estimation with other density estimators. To this end, our report consists of two main sections. In the first section, we run a one-shot experiment using kernel density estimation and its competitors, including average shifted histogram and penalized likelihood estimation, based specified data generating process. In the second part, we conduct a Monte Carlo simulation study for different sample sizes and observe how the estimation accuracy changes when we increase the sample size by reporting the corresponding integrated squared errors. 

## 2. Data Generating Processes and Preliminary Experiments

In this section, we introduce the methodologies of the kernel density estimation and its competitors first, before any data simulation or experiment. 

### 2.1 Methodology

**Kernel Density Estimation**
  
The kernel density estimation (KSE) is a widely-used density estimator. Comparing with histogram approaches, the kernel density estimation overcomes the discreteness of them by centring a smooth kernel function at each data point then summing to get a density estimate. 

Let $X_1, \cdots, X_n \sim^{iid} f$. The kernel estimator of $f$ is defined as 
$$\hat{f}_{kde}(x) = \frac{1}{n}\sum\limits_{i=1}^{n}K_h(x - X_i)=\frac{1}{n}\sum\limits_{i=1}^{n}K\bigg(\frac{x-X_i}{h}\bigg) $$
where $K$ is the density kernel and $h$ is the bandwidth. 

The kernel, $K$, is a symmetric, and usually positive function that integrates to one. Common kernel functions are uniform, triangle, Epanechnikov, quartic (biweight), tricube (triweight), Gaussian (normal), and cosine. 

The bandwidth, $h$, is a smoothing parameter. Generally, large bandwidths produce very smooth estimates, while small values produce wiggly estimates. The bandwidth influences estimation accuracy much more than the kernel, so choosing a good bandwidth is critical to get a good estimate.

Typically, we select the bandwidth of KDE and evaluate its performance by the integrated square error (ISE):
$$ISE = \int\big[\hat{f}_{kde}(x) - f(x) \big]^2dx$$
Lower value of ISE suggests better selection on the bandwidth and better performance of the estimator. However, this optimization problem can be difficult to solve as usually the true density function $f(x)$ is unknown. General consensus is that plug-in selectors and cross validation selectors are the most useful over a wide range of inputs.
  
**Average Shifted Histogram**

The histogram is the oldest and least sophisticated method for density estimation. One of its simple enhancement is the average shifted histogram (ASH), which is smoother and avoids the sensitivity to the choice of origin. Specifically, the premise of this approach is to take $m$ histograms, $\hat{f}_1, \hat{f}_2, \cdots, \hat{f}_m$, of bin width $h$ with origins of $t_o = 0, \frac{h}{m}, \frac{2h}{m}, \cdots, \frac{(m-1)h}{m}$. Based on these, we simply define the naive ASH as:
$$\hat{f}_{ash}(x) =  \frac{1}{m} \sum\limits_{i=1}^{m}\hat{f}_i(x)$$
There are $k = 1, \cdots, m \cdot n$ bins across all histograms, each spanning $\big[k\frac{h}{m}, (k+1)\frac{h}{m}\big]$ with centre $(k+0.5)\frac{h}{m}$. The ASH can be somewhat more general by using all bins to estimate the density at each point, weighting bins closer to the data more highly. In this way, ASH is also considered as an improvement for the discreteness of the traditional histogram. The weighted ASH is defined as:
$$\hat{f}_{ash}(x) =  \frac{1}{m} \sum\limits_{k=1}^{m \cdot n}\omega(l_k - x)\hat{c}_k(x)$$
where $\omega$ is a weighting function, $l_k$ is the centre of bin $k$, and $\hat{c}_k(x)$ is the number of points in that bin.
 
**Penalized Likelihood Estimation**

Penalized likelihood estimation (PLE) is another advanced approach to estimate the density, which considered the density as a mixture of $m$ "basis" densities, compromising between estimation accuracy and model complexity. As mentioned, PLE generally approximates the density of $x$, $f(x)$, as a mixture of $m$ densities:
$$\hat{f}_{pen}(x) = \sum\limits_{i=1}^{m}c_i\phi_i(x)$$
where $\phi_i(x)$ is a "basis" density and $c_i$ is the corresponding weight picked to ensure that $\hat{f}_{pen}$ can integrate to $1$. The basis densities are weighted equally and differ only by a location parameter, $\mu_i$. Thus, we can obtain a simplified definition which has a similar format with the kernel approach:
$$\hat{f}_{pen}(x) = \frac{1}{m}\sum\limits_{i=1}^{m}K\bigg(\frac{x-\mu_i}{h}\bigg) $$
Note that the "basis" densities are no longer constrained to be centred on the data points inn PLE, compared with KDE.

Determining the number and location of $\mu_i$ appropriately or not decides the performance of the penalized likelihood estimation. A general way is placing a large number of $\mu_i$ at equally spaced locations along the domain of the data and then minimizing a penalized likelihood to remove $\mu_i$ with little contribution to the overall quality.

### 2.2 Data Generating Processes


```{r echo = FALSE}
# set random seed
set.seed(1)

# data generating for one-shot experiments
data_1 <- rnorm(n = 500, mean = 5, sd = 3)              # Normal(5, 3^2)
data_2 <- rbeta(n = 500, shape1 = 1, shape2 = 10)       # Beta(1, 10)
```

```{r include = FALSE}
# data_1 ~ Normal(5, 9)
# Average Shifted Histogram
ash_1 <- ash1(bin1(data_1, ab = c(range(data_1)[1], 
                                  range(data_1)[2]), nbin = 50))

# Penalized Likelihood Estimation
ple_1 <- ssden(~ data_1, domain = data.frame(data_1 = c(range(data_1)[1],
                                                        range(data_1)[2])))
```

```{r echo = FALSE, fig.cap = "Estimated densities for Normal(5, 9) with different estimators"}
# visualize the accurate density and estimations
x_1 <- seq(range(data_1)[1], range(data_1)[2], length = 500)
# histogram
hist(data_1, freq = FALSE, xlab = "x", ylim = c(0, 0.15))
# accurate density
lines(x_1, dnorm(x_1, mean = 5, sd = 3), lwd = 2, col = "orange")
# Kernel Density Estimates
lines(density(data_1, from = range(data_1)[1], to = range(data_1)[2]), 
      lwd = 2, col = "red")
rug(jitter(data_1), side = 1, col = "red")
# Average Shifted Estimates
lines(ash_1, lwd = 2, col = "green")
# Penalized Likelihood Estimates
lines(x_1, dssden(ple_1, x_1), lwd = 2, col = "blue")
# legend
legend("topright", legend = c("Accurate density", "Kernel density estimation",
                              "Average shifted histogram", 
                              "Penalized density estimation"),
       lwd = c(2, 2, 2, 2), col = c("orange", "red", "green", "blue"), 
       bty = "n")
```

## 3. Monte Carlo Simulation Study

```{r, include = FALSE}
# data_1 ~ Normal(5, 9)

# function to compute ISE for both density estimation methods
ise_norm <- function(n){
  # This function is defined to get the Integrated Squared Error for both 
  # Kernel Density Estimation and Penalized Likelihood Estimation 
  # based on data from Normal(5, 9) distribution
  
  # simulated data from Normal(5, 9)
  x <- rnorm(n, mean = 5, sd = 3)
  
  # Kernel Density Estimation
  # compute the kernel density estimates
  kde <- kdensity(x, bw = "nrd0", kernel = "gaussian", 
                  support = c(min(x), max(x)))
  # compute the corresponding squared errors
  se_kde <- function(x) (kde(x) - dnorm(x, 5, 3))^2
  # compute ISE
  ise_kde <- integrate(se_kde, lower = min(x), upper = max(x))$value
  
  # Average Shifted Histogram
  # compute the ASH estimates
  ash <- ash1(bin1(x, ab = c(min(x), max(x)), nbin = 50))
  # compute the corresponding squared errors
  se_ash <- splinefun(ash$x, (ash$y - dnorm(ash$x, 5, 3))^2)
  # compute ISE 
  ise_ash <- integrate(se_ash, lower = min(x), upper = max(x))$value
  
  # Penalized Likelihood Estimation
  # compute the penalized likelihood estimates
  ple <- ssden(~x, domain = data.frame(x = c(min(x), max(x))))
  # compute the corresponding squared errors
  se_ple <- function(x) (dssden(ple, x) - dnorm(x, 5, 3))^2
  # compute ISE
  ise_ple <- integrate(se_ple, lower = min(x), upper = max(x))$value
  
  return(c(ise_kde, ise_ash, ise_ple))
  
}

set.seed(1)
# sample size = 250
ise_250 <- matrix(0, nrow = 100, ncol = 3)
for (i in 1:100){
  ise_250[i, 1] <- ise_norm(250)[1]
  ise_250[i, 2] <- ise_norm(250)[2]
  ise_250[i, 3] <- ise_norm(250)[3]
}

# sample size = 500
ise_500 <- matrix(0, nrow = 100, ncol = 3)
for (i in 1:100){
  ise_500[i, 1] <- ise_norm(500)[1]
  ise_500[i, 2] <- ise_norm(500)[2]
  ise_500[i, 3] <- ise_norm(500)[3]
}

# sample size = 1000
ise_1000 <- matrix(0, nrow = 100, ncol = 3)
for (i in 1:100){
  ise_1000[i, 1] <- ise_norm(1000)[1]
  ise_1000[i, 2] <- ise_norm(1000)[2]
  ise_1000[i, 3] <- ise_norm(1000)[3]
}
```

```{r, echo = FALSE}
# box-plots for ISE based on different methods
par(mfrow = c(1, 3))
boxplot(ise_250 ~ col(ise_250), names = c("KDE", "ASH", "PLE"), 
        col = c("gray", "green", "lightblue"), main = "sample size = 250",
        xlab = "estimator", ylab = "ise")
boxplot(ise_500 ~ col(ise_500), names = c("KDE", "ASH", "PLE"), 
        col = c("gray", "green", "lightblue"), main = "sample size = 500",
        xlab = "estimator", ylab = "ise")
boxplot(ise_1000 ~ col(ise_1000), names = c("KDE", "ASH", "PLE"), 
        col = c("gray", "green", "lightblue"), main = "sample size = 1000",
        xlab = "estimator", ylab = "ise")

# line graphs for mean ISE with CIs
# mean ISE for each method 
ise_250_mean <- apply(ise_250, 2, mean)
ise_500_mean <- apply(ise_500, 2, mean)
ise_1000_mean <- apply(ise_1000, 2, mean)

# standard deviations for ISEs
ise_250_sd <- apply(ise_250, 2, sd)
ise_500_sd <- apply(ise_500, 2, sd)
ise_1000_sd <- apply(ise_1000, 2, sd)

# data frame for critical values of ISEs
rep_ise <- data.frame(size = c(250, 500, 1000), 
                      mean_kse = c(ise_250_mean[1], ise_500_mean[1],
                                   ise_1000_mean[1]),
                      CI_kse_lower = c(ise_250_mean[1] - 1.96*ise_250_sd[1],
                                       ise_500_mean[1] - 1.96*ise_500_sd[1],
                                       ise_1000_mean[1] - 1.96*ise_1000_sd[1]),
                      CI_kse_upper = c(ise_250_mean[1] + 1.96*ise_250_sd[1],
                                       ise_500_mean[1] + 1.96*ise_500_sd[1],
                                       ise_1000_mean[1] + 1.96*ise_1000_sd[1]),
                      mean_ash = c(ise_250_mean[2], ise_500_mean[2],
                                   ise_1000_mean[2]),
                      CI_ash_lower = c(ise_250_mean[2] - 1.96*ise_250_sd[2],
                                       ise_500_mean[2] - 1.96*ise_500_sd[2],
                                       ise_1000_mean[2] - 1.96*ise_1000_sd[2]),
                      CI_ash_upper = c(ise_250_mean[2] + 1.96*ise_250_sd[2],
                                       ise_500_mean[2] + 1.96*ise_500_sd[2],
                                       ise_1000_mean[2] + 1.96*ise_1000_sd[2]),
                      mean_ple = c(ise_250_mean[3], ise_500_mean[3],
                                   ise_1000_mean[3]),
                      CI_ple_lower = c(ise_250_mean[3] - 1.96*ise_250_sd[3],
                                       ise_500_mean[3] - 1.96*ise_500_sd[3],
                                       ise_1000_mean[3] - 1.96*ise_1000_sd[3]),
                      CI_ple_upper = c(ise_250_mean[3] + 1.96*ise_250_sd[3],
                                       ise_500_mean[3] + 1.96*ise_500_sd[3],
                                       ise_1000_mean[3] + 1.96*ise_1000_sd[3]))

# make the plot
plot <- ggplot(rep_ise, aes(x = size)) +
  geom_point(aes(y = mean_kse, color = "mean ISE for KSE"), size = 1.25) + 
  geom_line(aes(y = mean_kse)) + 
  geom_ribbon(aes(ymin = CI_kse_lower, ymax = CI_kse_upper, 
                  fill = "95% CI for KSE"), alpha = 0.3) + 
  geom_point(aes(y = mean_ash, color = "mean ISE for ASH"), size = 1.25) + 
  geom_line(aes(y = mean_ash)) + 
  geom_ribbon(aes(ymin = CI_ash_lower, ymax = CI_ash_upper, 
                  fill = "95% CI for ASH"), alpha = 0.3) + 
  geom_point(aes(y = mean_ple, color = "mean ISE for PLE"), size = 1.25) + 
  geom_line(aes(y = mean_ple)) + 
  geom_ribbon(aes(ymin = CI_ple_lower, ymax = CI_ple_upper, 
                  fill = "95% CI for PLE"), alpha = 0.3) + 
  ggtitle("Critical values for mean ISE of different methods") + 
  theme(plot.title = element_text(hjust = 0.5)) + 
  labs(x = "sample size", y = "ISE", color = "", fill = "") 

plot
```


## References

